
Open In Colab
import pandas as pd 
import numpy as np
from google.colab import drive
drive.mount('/content/drive')
Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly

Enter your authorization code:
··········
Mounted at /content/drive
DrDoS_DNS_data_1_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_DNS_data_1_per.csv')
DrDoS_LDAP_data_2_0_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_LDAP_data_2_0_per.csv')
DrDoS_MSSQL_data_1_3_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_MSSQL_data_1_3_per.csv')
DrDoS_NetBIOS_data_1_3_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_NetBIOS_data_1_3_per.csv')
DrDoS_NTP_data_data_5_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_NTP_data_data_5_per.csv')
DrDoS_SNMP_data_1_3_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_SNMP_data_1_3_per.csv')
DrDoS_SSDP_data_2_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_SSDP_data_2_per.csv')
DrDoS_UDP_data_2_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/DrDoS_UDP_data_2_per.csv')
Syn_data_4_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/syn_data.csv')
UDPLag_data_2_0_per = pd.read_csv('/content/drive/My Drive/DDos_Dataset/UDPLag_data_2_0_per.csv')
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (86) have mixed types.Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
# Merge all the Dataset to make one data 
data = pd.concat([DrDoS_DNS_data_1_per, DrDoS_LDAP_data_2_0_per, DrDoS_MSSQL_data_1_3_per, DrDoS_NetBIOS_data_1_3_per, DrDoS_NTP_data_data_5_per, DrDoS_SNMP_data_1_3_per, DrDoS_SSDP_data_2_per, DrDoS_UDP_data_2_per, Syn_data_4_per, UDPLag_data_2_0_per], ignore_index = True)
data.shape
(585970, 89)
data[' Label'].value_counts()
DrDoS_SNMP       67076
UDP-lag          66072
DrDoS_UDP        62702
DrDoS_NTP        60129
DrDoS_MSSQL      58802
Syn              58347
DrDoS_LDAP       54496
DrDoS_NetBIOS    53214
DrDoS_SSDP       52216
DrDoS_DNS        50715
BENIGN            2122
WebDDoS             79
Name:  Label, dtype: int64
# Drop Unnamed:0, Unnamed:0.1 columns 
data = data.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1)
data.columns 
Index(['Flow ID', ' Source IP', ' Source Port', ' Destination IP',
       ' Destination Port', ' Protocol', ' Timestamp', ' Flow Duration',
       ' Total Fwd Packets', ' Total Backward Packets',
       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',
       ' Fwd Packet Length Max', ' Fwd Packet Length Min',
       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',
       'Bwd Packet Length Max', ' Bwd Packet Length Min',
       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',
       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',
       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',
       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',
       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',
       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',
       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',
       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',
       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',
       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',
       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',
       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',
       ' Average Packet Size', ' Avg Fwd Segment Size',
       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',
       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',
       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',
       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',
       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',
       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',
       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',
       ' Idle Max', ' Idle Min', 'SimillarHTTP', ' Inbound', ' Label'],
      dtype='object')
data_real = data.replace(np.inf, np.nan)
data_real.isnull().sum().sum()
23612
data_df = data_real.dropna(axis=0)
data_df.isnull().sum().sum()
0
data_df
Flow ID	Source IP	Source Port	Destination IP	Destination Port	Protocol	Timestamp	Flow Duration	Total Fwd Packets	Total Backward Packets	Total Length of Fwd Packets	Total Length of Bwd Packets	Fwd Packet Length Max	Fwd Packet Length Min	Fwd Packet Length Mean	Fwd Packet Length Std	Bwd Packet Length Max	Bwd Packet Length Min	Bwd Packet Length Mean	Bwd Packet Length Std	Flow Bytes/s	Flow Packets/s	Flow IAT Mean	Flow IAT Std	Flow IAT Max	Flow IAT Min	Fwd IAT Total	Fwd IAT Mean	Fwd IAT Std	Fwd IAT Max	Fwd IAT Min	Bwd IAT Total	Bwd IAT Mean	Bwd IAT Std	Bwd IAT Max	Bwd IAT Min	Fwd PSH Flags	Bwd PSH Flags	Fwd URG Flags	Bwd URG Flags	...	Packet Length Std	Packet Length Variance	FIN Flag Count	SYN Flag Count	RST Flag Count	PSH Flag Count	ACK Flag Count	URG Flag Count	CWE Flag Count	ECE Flag Count	Down/Up Ratio	Average Packet Size	Avg Fwd Segment Size	Avg Bwd Segment Size	Fwd Header Length.1	Fwd Avg Bytes/Bulk	Fwd Avg Packets/Bulk	Fwd Avg Bulk Rate	Bwd Avg Bytes/Bulk	Bwd Avg Packets/Bulk	Bwd Avg Bulk Rate	Subflow Fwd Packets	Subflow Fwd Bytes	Subflow Bwd Packets	Subflow Bwd Bytes	Init_Win_bytes_forward	Init_Win_bytes_backward	act_data_pkt_fwd	min_seg_size_forward	Active Mean	Active Std	Active Max	Active Min	Idle Mean	Idle Std	Idle Max	Idle Min	SimillarHTTP	Inbound	Label
0	172.16.0.5-192.168.50.1-556-16923-17	172.16.0.5	556	192.168.50.1	16923	17	2018-12-01 11:13:44.496470	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	2144.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	1072.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	DrDoS_DNS
1	172.16.0.5-192.168.50.1-690-38772-17	172.16.0.5	690	192.168.50.1	38772	17	2018-12-01 11:12:47.056822	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	DrDoS_DNS
2	172.16.0.5-192.168.50.1-761-24941-17	172.16.0.5	761	192.168.50.1	24941	17	2018-12-01 11:12:58.063304	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	DrDoS_DNS
3	172.16.0.5-192.168.50.1-526-22632-17	172.16.0.5	526	192.168.50.1	22632	17	2018-12-01 11:10:57.969713	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	DrDoS_DNS
4	172.16.0.5-192.168.50.1-546-5185-17	172.16.0.5	546	192.168.50.1	5185	17	2018-12-01 11:11:40.592201	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	2280.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	1140.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	DrDoS_DNS
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
585965	172.16.0.5-192.168.50.1-26917-29675-6	172.16.0.5	26917	192.168.50.1	29675	6	2018-12-01 13:30:12.041041	104.0	2.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	3.846154e+04	3.466667e+01	5.831238e+01	102.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	1.0	1.0	0.0	1.0	1.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	40.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	0.0	2.0	0.0	5840.0	0.0	0.0	20.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	UDP-lag
585966	172.16.0.5-192.168.50.1-33856-60933-17	172.16.0.5	33856	192.168.50.1	60933	17	2018-12-01 13:04:55.462724	2.0	2.0	0.0	750.0	0.0	375.0	375.0	375.0	0.0	0.0	0.0	0.0	0.0	3.750000e+08	1.000000e+06	2.000000e+00	0.000000e+00	2.0	2.0	2.0	2.0	0.000000e+00	2.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	562.5	375.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	750.0	0.0	0.0	-1.0	-1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	UDP-lag
585967	172.16.0.5-192.168.50.1-10655-18473-6	172.16.0.5	10655	192.168.50.1	18473	6	2018-12-01 13:30:08.999264	50.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	4.000000e+04	5.000000e+01	0.000000e+00	50.0	50.0	50.0	50.0	0.000000e+00	50.0	50.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	40.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	0	1.0	UDP-lag
585968	172.16.0.5-192.168.50.1-63789-3373-6	172.16.0.5	63789	192.168.50.1	3373	6	2018-12-01 13:29:53.212560	33957023.0	6.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	1.766939e-01	6.791405e+06	9.312966e+06	17686187.0	1.0	33957023.0	6791404.6	9.312966e+06	17686187.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	120.0	0.0	0.0	0.0	0.0	0.0	0.0	6.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	1.0	0.0	1.0	1.0	16978510.0	1.000806e+06	17686187.0	16270833.0	0	1.0	UDP-lag
585969	172.16.0.5-192.168.50.1-62675-62675-6	172.16.0.5	62675	192.168.50.1	62675	6	2018-12-01 13:30:06.551846	16621605.0	6.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	3.609760e-01	3.324321e+06	6.835984e+06	15523271.0	0.0	16621605.0	3324321.0	6.835984e+06	15523271.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	120.0	0.0	0.0	0.0	0.0	0.0	0.0	6.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	1098333.0	0.0	1098333.0	1098333.0	15523271.0	0.000000e+00	15523271.0	15523271.0	0	1.0	UDP-lag
574164 rows × 87 columns

# data_df.to_csv('data_final.csv', index = False)
# from google.colab import files
# files.download('data_final.csv')
data_X = data_df.drop([' Label', 'SimillarHTTP'], axis = 1)
data_X.columns 
Index(['Flow ID', ' Source IP', ' Source Port', ' Destination IP',
       ' Destination Port', ' Protocol', ' Timestamp', ' Flow Duration',
       ' Total Fwd Packets', ' Total Backward Packets',
       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',
       ' Fwd Packet Length Max', ' Fwd Packet Length Min',
       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',
       'Bwd Packet Length Max', ' Bwd Packet Length Min',
       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',
       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',
       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',
       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',
       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',
       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',
       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',
       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',
       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',
       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',
       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',
       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',
       ' Average Packet Size', ' Avg Fwd Segment Size',
       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',
       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',
       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',
       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',
       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',
       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',
       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',
       ' Idle Max', ' Idle Min', ' Inbound'],
      dtype='object')
data_X.shape 
(574164, 85)
data_y = data_df[' Label']
data_y.shape 
(574164,)
data_df.isnull().sum().sum()
0
data_y.unique()
array(['DrDoS_DNS', 'BENIGN', 'DrDoS_LDAP', 'DrDoS_MSSQL',
       'DrDoS_NetBIOS', 'DrDoS_NTP', 'DrDoS_SNMP', 'DrDoS_SSDP',
       'DrDoS_UDP', 'Syn', 'UDP-lag', 'WebDDoS'], dtype=object)
data_X 
Flow ID	Source IP	Source Port	Destination IP	Destination Port	Protocol	Timestamp	Flow Duration	Total Fwd Packets	Total Backward Packets	Total Length of Fwd Packets	Total Length of Bwd Packets	Fwd Packet Length Max	Fwd Packet Length Min	Fwd Packet Length Mean	Fwd Packet Length Std	Bwd Packet Length Max	Bwd Packet Length Min	Bwd Packet Length Mean	Bwd Packet Length Std	Flow Bytes/s	Flow Packets/s	Flow IAT Mean	Flow IAT Std	Flow IAT Max	Flow IAT Min	Fwd IAT Total	Fwd IAT Mean	Fwd IAT Std	Fwd IAT Max	Fwd IAT Min	Bwd IAT Total	Bwd IAT Mean	Bwd IAT Std	Bwd IAT Max	Bwd IAT Min	Fwd PSH Flags	Bwd PSH Flags	Fwd URG Flags	Bwd URG Flags	...	Max Packet Length	Packet Length Mean	Packet Length Std	Packet Length Variance	FIN Flag Count	SYN Flag Count	RST Flag Count	PSH Flag Count	ACK Flag Count	URG Flag Count	CWE Flag Count	ECE Flag Count	Down/Up Ratio	Average Packet Size	Avg Fwd Segment Size	Avg Bwd Segment Size	Fwd Header Length.1	Fwd Avg Bytes/Bulk	Fwd Avg Packets/Bulk	Fwd Avg Bulk Rate	Bwd Avg Bytes/Bulk	Bwd Avg Packets/Bulk	Bwd Avg Bulk Rate	Subflow Fwd Packets	Subflow Fwd Bytes	Subflow Bwd Packets	Subflow Bwd Bytes	Init_Win_bytes_forward	Init_Win_bytes_backward	act_data_pkt_fwd	min_seg_size_forward	Active Mean	Active Std	Active Max	Active Min	Idle Mean	Idle Std	Idle Max	Idle Min	Inbound
0	172.16.0.5-192.168.50.1-556-16923-17	172.16.0.5	556	192.168.50.1	16923	17	2018-12-01 11:13:44.496470	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	2144.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	1072.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
1	172.16.0.5-192.168.50.1-690-38772-17	172.16.0.5	690	192.168.50.1	38772	17	2018-12-01 11:12:47.056822	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
2	172.16.0.5-192.168.50.1-761-24941-17	172.16.0.5	761	192.168.50.1	24941	17	2018-12-01 11:12:58.063304	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
3	172.16.0.5-192.168.50.1-526-22632-17	172.16.0.5	526	192.168.50.1	22632	17	2018-12-01 11:10:57.969713	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
4	172.16.0.5-192.168.50.1-546-5185-17	172.16.0.5	546	192.168.50.1	5185	17	2018-12-01 11:11:40.592201	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	2280.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	1140.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
585965	172.16.0.5-192.168.50.1-26917-29675-6	172.16.0.5	26917	192.168.50.1	29675	6	2018-12-01 13:30:12.041041	104.0	2.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	3.846154e+04	3.466667e+01	5.831238e+01	102.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	1.0	1.0	0.0	1.0	1.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	40.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	0.0	2.0	0.0	5840.0	0.0	0.0	20.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
585966	172.16.0.5-192.168.50.1-33856-60933-17	172.16.0.5	33856	192.168.50.1	60933	17	2018-12-01 13:04:55.462724	2.0	2.0	0.0	750.0	0.0	375.0	375.0	375.0	0.0	0.0	0.0	0.0	0.0	3.750000e+08	1.000000e+06	2.000000e+00	0.000000e+00	2.0	2.0	2.0	2.0	0.000000e+00	2.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	375.0	375.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	562.5	375.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	750.0	0.0	0.0	-1.0	-1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
585967	172.16.0.5-192.168.50.1-10655-18473-6	172.16.0.5	10655	192.168.50.1	18473	6	2018-12-01 13:30:08.999264	50.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	4.000000e+04	5.000000e+01	0.000000e+00	50.0	50.0	50.0	50.0	0.000000e+00	50.0	50.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	40.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
585968	172.16.0.5-192.168.50.1-63789-3373-6	172.16.0.5	63789	192.168.50.1	3373	6	2018-12-01 13:29:53.212560	33957023.0	6.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	1.766939e-01	6.791405e+06	9.312966e+06	17686187.0	1.0	33957023.0	6791404.6	9.312966e+06	17686187.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	120.0	0.0	0.0	0.0	0.0	0.0	0.0	6.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	1.0	0.0	1.0	1.0	16978510.0	1.000806e+06	17686187.0	16270833.0	1.0
585969	172.16.0.5-192.168.50.1-62675-62675-6	172.16.0.5	62675	192.168.50.1	62675	6	2018-12-01 13:30:06.551846	16621605.0	6.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	3.609760e-01	3.324321e+06	6.835984e+06	15523271.0	0.0	16621605.0	3324321.0	6.835984e+06	15523271.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	120.0	0.0	0.0	0.0	0.0	0.0	0.0	6.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	1098333.0	0.0	1098333.0	1098333.0	15523271.0	0.000000e+00	15523271.0	15523271.0	1.0
574164 rows × 85 columns

Label Encoding for the Dataset
from sklearn.preprocessing import LabelEncoder 
le = LabelEncoder()
data_y_trans = le.fit_transform(data_y)
data_y_trans
array([ 1,  1,  1, ..., 10, 10, 10])
le_fid = LabelEncoder()
le_fid.fit(data_X['Flow ID'])
data_X['Flow ID'] = le_fid.fit_transform(data_X['Flow ID'])
le_SIP = LabelEncoder()
le_SIP.fit(data_X[' Source IP'])
data_X[' Source IP'] = le_SIP.fit_transform(data_X[' Source IP'])
le_DIP = LabelEncoder()
le_DIP.fit(data_X[' Destination IP'])
data_X[' Destination IP'] = le_DIP.fit_transform(data_X[' Destination IP'])
le_timestamp = LabelEncoder()
le_timestamp.fit(data_X[' Timestamp'])
data_X[' Timestamp'] = le_timestamp.fit_transform(data_X[' Timestamp'])
data_X
Flow ID	Source IP	Source Port	Destination IP	Destination Port	Protocol	Timestamp	Flow Duration	Total Fwd Packets	Total Backward Packets	Total Length of Fwd Packets	Total Length of Bwd Packets	Fwd Packet Length Max	Fwd Packet Length Min	Fwd Packet Length Mean	Fwd Packet Length Std	Bwd Packet Length Max	Bwd Packet Length Min	Bwd Packet Length Mean	Bwd Packet Length Std	Flow Bytes/s	Flow Packets/s	Flow IAT Mean	Flow IAT Std	Flow IAT Max	Flow IAT Min	Fwd IAT Total	Fwd IAT Mean	Fwd IAT Std	Fwd IAT Max	Fwd IAT Min	Bwd IAT Total	Bwd IAT Mean	Bwd IAT Std	Bwd IAT Max	Bwd IAT Min	Fwd PSH Flags	Bwd PSH Flags	Fwd URG Flags	Bwd URG Flags	...	Max Packet Length	Packet Length Mean	Packet Length Std	Packet Length Variance	FIN Flag Count	SYN Flag Count	RST Flag Count	PSH Flag Count	ACK Flag Count	URG Flag Count	CWE Flag Count	ECE Flag Count	Down/Up Ratio	Average Packet Size	Avg Fwd Segment Size	Avg Bwd Segment Size	Fwd Header Length.1	Fwd Avg Bytes/Bulk	Fwd Avg Packets/Bulk	Fwd Avg Bulk Rate	Bwd Avg Bytes/Bulk	Bwd Avg Packets/Bulk	Bwd Avg Bulk Rate	Subflow Fwd Packets	Subflow Fwd Bytes	Subflow Bwd Packets	Subflow Bwd Bytes	Init_Win_bytes_forward	Init_Win_bytes_backward	act_data_pkt_fwd	min_seg_size_forward	Active Mean	Active Std	Active Max	Active Min	Idle Mean	Idle Std	Idle Max	Idle Min	Inbound
0	205418	17	556	129	16923	17	153807	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	2144.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	1072.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
1	356277	17	690	129	38772	17	149038	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
2	390438	17	761	129	24941	17	149918	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
3	172900	17	526	129	22632	17	139638	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	-2.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	-1.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
4	194820	17	546	129	5185	17	143350	1.0	2.0	0.0	2944.0	0.0	1472.0	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	2.944000e+09	2.000000e+06	1.000000e+00	0.000000e+00	1.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1472.0	1472.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2208.0	1472.0	0.0	2280.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	2944.0	0.0	0.0	-1.0	-1.0	1.0	1140.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
585965	38260	17	26917	129	29675	6	555528	104.0	2.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	3.846154e+04	3.466667e+01	5.831238e+01	102.0	1.0	1.0	1.0	0.000000e+00	1.0	1.0	1.0	1.0	0.0	1.0	1.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	40.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	0.0	2.0	0.0	5840.0	0.0	0.0	20.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
585966	54237	17	33856	129	60933	17	514638	2.0	2.0	0.0	750.0	0.0	375.0	375.0	375.0	0.0	0.0	0.0	0.0	0.0	3.750000e+08	1.000000e+06	2.000000e+00	0.000000e+00	2.0	2.0	2.0	2.0	0.000000e+00	2.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	375.0	375.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	562.5	375.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	750.0	0.0	0.0	-1.0	-1.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
585967	11899	17	10655	129	18473	6	552548	50.0	2.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	4.000000e+04	5.000000e+01	0.000000e+00	50.0	50.0	50.0	50.0	0.000000e+00	50.0	50.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	40.0	0.0	0.0	0.0	0.0	0.0	0.0	2.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	0.0	0.0	1.0
585968	309616	17	63789	129	3373	6	534101	33957023.0	6.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	1.766939e-01	6.791405e+06	9.312966e+06	17686187.0	1.0	33957023.0	6791404.6	9.312966e+06	17686187.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	120.0	0.0	0.0	0.0	0.0	0.0	0.0	6.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	1.0	0.0	1.0	1.0	16978510.0	1.000806e+06	17686187.0	16270833.0	1.0
585969	292092	17	62675	129	62675	6	549955	16621605.0	6.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.000000e+00	3.609760e-01	3.324321e+06	6.835984e+06	15523271.0	0.0	16621605.0	3324321.0	6.835984e+06	15523271.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	120.0	0.0	0.0	0.0	0.0	0.0	0.0	6.0	0.0	0.0	0.0	5840.0	-1.0	0.0	20.0	1098333.0	0.0	1098333.0	1098333.0	15523271.0	0.000000e+00	15523271.0	15523271.0	1.0
574164 rows × 85 columns

data_X.dtypes
Flow ID                int64
 Source IP             int64
 Source Port           int64
 Destination IP        int64
 Destination Port      int64
                      ...   
Idle Mean            float64
 Idle Std            float64
 Idle Max            float64
 Idle Min            float64
 Inbound             float64
Length: 85, dtype: object
Feature Selection
from sklearn.feature_selection import chi2 
from sklearn.feature_selection import SelectKBest 
from sklearn.ensemble import ExtraTreesClassifier

#selecting 20 best features
# select_best= SelectKBest(chi2, k=20)
# X_feat_20 = select_best.fit_transform(data_X, data_y_trans)
# X_feat_20.shape

model = ExtraTreesClassifier(random_state=42)
model.fit(data_X, data_y_trans)
ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_impurity_split=None,
                     min_samples_leaf=1, min_samples_split=2,
                     min_weight_fraction_leaf=0.0, n_estimators=100,
                     n_jobs=None, oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
model.feature_importances_
array([4.55691632e-02, 1.05536287e-03, 6.07409979e-02, 4.58689181e-03,
       1.09243515e-02, 2.69234462e-02, 3.18773279e-01, 2.12886562e-03,
       4.94245245e-03, 1.89339061e-04, 1.55905431e-02, 4.16611954e-05,
       3.93580036e-02, 5.01477293e-02, 3.56746039e-02, 3.33406045e-03,
       4.44009140e-04, 2.17114614e-03, 3.51474416e-04, 3.05095572e-05,
       3.08077317e-02, 1.57018741e-02, 3.50672156e-03, 1.95113249e-03,
       3.01241816e-03, 2.95024968e-03, 2.74855882e-03, 3.00954810e-03,
       2.14181795e-03, 3.36396944e-03, 3.01320312e-03, 1.65141303e-04,
       2.48395815e-04, 1.40391633e-04, 1.60190778e-04, 2.66552490e-04,
       8.40458843e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       6.10587679e-03, 1.65195799e-04, 1.71172483e-02, 3.66112904e-04,
       5.07671691e-02, 2.94632407e-02, 4.08119261e-02, 1.57542792e-03,
       1.11100325e-03, 0.00000000e+00, 1.15744663e-05, 5.27714114e-05,
       0.00000000e+00, 3.69356260e-02, 5.04022416e-04, 1.73879536e-04,
       0.00000000e+00, 2.00405466e-03, 3.87984079e-02, 3.58090171e-02,
       9.00962226e-05, 6.01877346e-03, 0.00000000e+00, 0.00000000e+00,
       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       5.30061319e-03, 1.10650481e-02, 1.04462235e-04, 2.31396614e-04,
       2.15309518e-03, 9.29948216e-05, 7.84427567e-03, 6.75823061e-03,
       2.79152528e-06, 1.29557293e-06, 7.22728427e-06, 4.38285607e-06,
       2.72179345e-04, 4.54329549e-05, 3.91616930e-04, 4.85452936e-04,
       1.10824731e-03])
feature_importance_std = pd.Series(model.feature_importances_, index=data_X.columns)
feature_importance_std.nlargest(20).plot(kind='bar', title='Standardised Dataset Feature Selection using ExtraTreesClassifier')
<matplotlib.axes._subplots.AxesSubplot at 0x7f915a36ca20>

data_X.shape 
(574164, 85)
data_new_20features_X = data_X[[' Timestamp', ' Source Port', ' Min Packet Length', ' Fwd Packet Length Min', 'Flow ID', ' Packet Length Mean', ' Fwd Packet Length Max', ' Average Packet Size', ' ACK Flag Count', ' Avg Fwd Segment Size', ' Fwd Packet Length Mean', 'Flow Bytes/s', ' Max Packet Length', ' Protocol', 'Fwd Packets/s', ' Flow Packets/s', 'Total Length of Fwd Packets', ' Subflow Fwd Bytes', ' Destination Port', ' act_data_pkt_fwd']]
data_new_20features_X
Timestamp	Source Port	Min Packet Length	Fwd Packet Length Min	Flow ID	Packet Length Mean	Fwd Packet Length Max	Average Packet Size	ACK Flag Count	Avg Fwd Segment Size	Fwd Packet Length Mean	Flow Bytes/s	Max Packet Length	Protocol	Fwd Packets/s	Flow Packets/s	Total Length of Fwd Packets	Subflow Fwd Bytes	Destination Port	act_data_pkt_fwd
0	153807	556	1472.0	1472.0	205418	1472.0	1472.0	2208.0	0.0	1472.0	1472.0	2.944000e+09	1472.0	17	2.000000e+06	2.000000e+06	2944.0	2944.0	16923	1.0
1	149038	690	1472.0	1472.0	356277	1472.0	1472.0	2208.0	0.0	1472.0	1472.0	2.944000e+09	1472.0	17	2.000000e+06	2.000000e+06	2944.0	2944.0	38772	1.0
2	149918	761	1472.0	1472.0	390438	1472.0	1472.0	2208.0	0.0	1472.0	1472.0	2.944000e+09	1472.0	17	2.000000e+06	2.000000e+06	2944.0	2944.0	24941	1.0
3	139638	526	1472.0	1472.0	172900	1472.0	1472.0	2208.0	0.0	1472.0	1472.0	2.944000e+09	1472.0	17	2.000000e+06	2.000000e+06	2944.0	2944.0	22632	1.0
4	143350	546	1472.0	1472.0	194820	1472.0	1472.0	2208.0	0.0	1472.0	1472.0	2.944000e+09	1472.0	17	2.000000e+06	2.000000e+06	2944.0	2944.0	5185	1.0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
585965	555528	26917	0.0	0.0	38260	0.0	0.0	0.0	1.0	0.0	0.0	0.000000e+00	0.0	6	1.923077e+04	3.846154e+04	0.0	0.0	29675	0.0
585966	514638	33856	375.0	375.0	54237	375.0	375.0	562.5	0.0	375.0	375.0	3.750000e+08	375.0	17	1.000000e+06	1.000000e+06	750.0	750.0	60933	1.0
585967	552548	10655	0.0	0.0	11899	0.0	0.0	0.0	1.0	0.0	0.0	0.000000e+00	0.0	6	4.000000e+04	4.000000e+04	0.0	0.0	18473	0.0
585968	534101	63789	0.0	0.0	309616	0.0	0.0	0.0	1.0	0.0	0.0	0.000000e+00	0.0	6	1.766939e-01	1.766939e-01	0.0	0.0	3373	0.0
585969	549955	62675	0.0	0.0	292092	0.0	0.0	0.0	1.0	0.0	0.0	0.000000e+00	0.0	6	3.609760e-01	3.609760e-01	0.0	0.0	62675	0.0
574164 rows × 20 columns

Train Test Split Normal dataset 84 Features
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_X, data_y_trans, test_size = 0.30, random_state = 42)
X_train.shape 
(401914, 85)
X_test.shape 
(172250, 85)
Standardization of the 84 Feature Dataset
from sklearn.preprocessing import StandardScaler 
ss = StandardScaler()
X_train_std = ss.fit_transform(X_train)
X_test_std = ss.fit_transform(X_test)
Train Test Split 20 Feature Dataset
from sklearn.model_selection import train_test_split
X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(data_new_20features_X, data_y_trans, test_size = 0.30, random_state = 42)
Standardization of the 20 Feature Dataset
from sklearn.preprocessing import StandardScaler 
ss_20 = StandardScaler()
X_train_std_20 = ss_20.fit_transform(X_train_20)
X_test_std_20 = ss_20.fit_transform(X_test_20)
X_train_std_20.shape 
(401914, 20)
y_train_20.shape
(401914,)
X_test_std_20.shape 
(172250, 20)
y_test_20.shape 
(172250,)
1. Random Forest Classification
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train_std_20, y_train_20)
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)
rf_y_pred = rf.predict(X_test_std_20)
rf_y_pred
array([ 4,  6,  4, ..., 10,  2, 10])
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report 
from sklearn.metrics import confusion_matrix 
print("Classification Report for Random Forest: \n", classification_report(le.inverse_transform(y_test_20), le.inverse_transform(rf_y_pred)))
Classification Report for Random Forest: 
                precision    recall  f1-score   support

       BENIGN       0.90      1.00      0.95       616
    DrDoS_DNS       0.98      1.00      0.99     14699
   DrDoS_LDAP       0.99      0.99      0.99     15992
  DrDoS_MSSQL       0.99      1.00      0.99     17017
    DrDoS_NTP       1.00      1.00      1.00     17886
DrDoS_NetBIOS       0.99      0.99      0.99     15529
   DrDoS_SNMP       0.99      0.99      0.99     20133
   DrDoS_SSDP       0.99      0.99      0.99     15562
    DrDoS_UDP       0.99      0.99      0.99     18565
          Syn       1.00      1.00      1.00     16340
      UDP-lag       1.00      0.99      1.00     19883
      WebDDoS       1.00      0.36      0.53        28

     accuracy                           0.99    172250
    macro avg       0.99      0.94      0.95    172250
 weighted avg       0.99      0.99      0.99    172250

rf_conf_mat = confusion_matrix(y_test_20, rf_y_pred)
print("Random Forest Confusion: \n", rf_conf_mat)
Random Forest Confusion: 
 [[  615     1     0     0     0     0     0     0     0     0     0     0]
 [    1 14656     0     0    36     6     0     0     0     0     0     0]
 [    3   207 15779     3     0     0     0     0     0     0     0     0]
 [    2     0    82 16932     1     0     0     0     0     0     0     0]
 [   24     0     0     0 17862     0     0     0     0     0     0     0]
 [    2    34     0   158     1 15333     1     0     0     0     0     0]
 [    5     2     0     0     0   184 19942     0     0     0     0     0]
 [    1     0     0     0     0     2   182 15376     1     0     0     0]
 [    2     0     0     0     0     1     0   170 18391     0     1     0]
 [    4     0     0     0     0     0     0     0     0 16336     0     0]
 [    5     0     0     0     0     0     0     0   185     0 19693     0]
 [   17     0     0     0     0     0     0     0     0     0     1    10]]
acc_score = accuracy_score(y_test_20, rf_y_pred)
print("Accuracy Score for Random_Forest: \n", acc_score*100)
Accuracy Score for Random_Forest: 
 99.23076923076923
# RoC curve Function 

def RoC_Curve(classifier, X_val, y_val, title): 
        """ RoC Curve for Classifier 
        Parameters: 
        ------------
        classifier: Machine Learning Classifier to be Evaluated
        X_val: Validation Dataset
        y_val: Label/Target of Validation Dataset

        Attributes:
        Plots the Graph    
        
        Note: Some part of this Method code is taken 
            from Sklearn Website
        """

        lw = 2
        n_classes = 12
        y_test1 = to_categorical(y_val)
        pred_RFC_proba = classifier.predict_proba(X_val)
        y_score = pred_RFC_proba

        # Compute ROC curve and ROC area for each class
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_test1[:, i], y_score[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        # Compute micro-average ROC curve and ROC area
        fpr["micro"], tpr["micro"], _ = roc_curve(y_test1.ravel(), y_score.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        # First aggregate all false positive rates
        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

        # Then interpolate all ROC curves at this points
        mean_tpr = np.zeros_like(all_fpr)
        for i in range(n_classes):
            mean_tpr += interp(all_fpr, fpr[i], tpr[i])

        # Finally average it and compute AUC
        mean_tpr /= n_classes

        fpr["macro"] = all_fpr
        tpr["macro"] = mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

        # Plot all ROC curves
        plt.figure(figsize=(20,10))
        plt.plot(fpr["micro"], tpr["micro"],
                label='micro-average ROC curve (area = {0:0.2f})'
                    ''.format(roc_auc["micro"]),
                color='deeppink', linestyle=':', linewidth=4)

        plt.plot(fpr["macro"], tpr["macro"],
                label='macro-average ROC curve (area = {0:0.2f})'
                    ''.format(roc_auc["macro"]),
                color='navy', linestyle=':', linewidth=4)

        list_class = ['BENIGN', 'DrDoS_DNS', 'DrDoS_LDAP', 'DrDoS_MSSQL', 'DrDoS_NTP', 'DrDoS_NetBIOS', 'DrDoS_SNMP', 'DrDoS_SSDP', 'DrDoS_UDP', 'Syn', 'UDP-lag', 'WebDDoS']
        for i in range(n_classes):
            plt.plot(fpr[i], tpr[i], lw=lw,
                    label='ROC curve of class {0} (area = {1:0.2f})'
                    ''.format(list_class[i], roc_auc[i]))

        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(title) 
        plt.legend(loc="lower right")
        plt.show()
from keras.utils.np_utils import to_categorical
from sklearn.metrics import roc_curve, auc
from scipy import interp
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt 
title = 'Receiver operating characteristic of Random Forest'
RoC_Curve(rf, X_test_std_20, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

2. Decision Tree
from sklearn.tree import DecisionTreeClassifier 
dt = DecisionTreeClassifier()
dt.fit(X_train_std_20, y_train_20)
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=None, splitter='best')
dt_y_pred = dt.predict(X_test_std_20)
print("Classification Report for Decision Tree: \n", classification_report(le.inverse_transform(y_test_20), le.inverse_transform(dt_y_pred)))
Classification Report for Decision Tree: 
                precision    recall  f1-score   support

       BENIGN       0.82      1.00      0.90       616
    DrDoS_DNS       0.99      0.99      0.99     14699
   DrDoS_LDAP       0.98      0.99      0.99     15992
  DrDoS_MSSQL       0.99      0.98      0.99     17017
    DrDoS_NTP       0.99      1.00      0.99     17886
DrDoS_NetBIOS       0.99      0.99      0.99     15529
   DrDoS_SNMP       0.99      0.99      0.99     20133
   DrDoS_SSDP       0.99      0.99      0.99     15562
    DrDoS_UDP       0.99      0.99      0.99     18565
          Syn       1.00      0.99      1.00     16340
      UDP-lag       1.00      0.99      0.99     19883
      WebDDoS       0.78      0.25      0.38        28

     accuracy                           0.99    172250
    macro avg       0.96      0.93      0.93    172250
 weighted avg       0.99      0.99      0.99    172250

dt_conf_mat = confusion_matrix(y_test_20, dt_y_pred)
print("Decision Tree Confusion: \n", dt_conf_mat)
Decision Tree Confusion: 
 [[  614     0     0     0     0     0     0     0     0     0     2     0]
 [    0 14502     0     0   197     0     0     0     0     0     0     0]
 [    0   204 15788     0     0     0     0     0     0     0     0     0]
 [    0     0   268 16749     0     0     0     0     0     0     0     0]
 [   38     0     0     0 17831     0     0     0     0    17     0     0]
 [    1     0     0   159     0 15369     0     0     0     0     0     0]
 [    0     0     0     0     0   186 19947     0     0     0     0     0]
 [    0     0     0     0     0     0   181 15381     0     0     0     0]
 [    0     0     0     0     0     0     0   168 18397     0     0     0]
 [   96     0     0     0     0     0     0     0     0 16244     0     0]
 [    0     0     0     0     0     0     0     0   183     0 19698     2]
 [    0     0     0     0     0     0     0     0     0     0    21     7]]
acc_score_dt = accuracy_score(y_test_20, dt_y_pred)
print("Accuracy Score for Decision Tree: \n", acc_score_dt*100)
Accuracy Score for Decision Tree: 
 98.99970972423803
# RoC Curve 
title = 'Receiver operating characteristic of Decision Tree'
RoC_Curve(dt, X_test_std_20, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

3. SVM
from sklearn.svm import LinearSVC
svm = LinearSVC(multi_class = 'ovr')
svm.fit(X_train_std_20, y_train_20)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
          verbose=0)
y_pred_svm = svm.predict(X_test_std_20) 
svm.score(X_test_std_20, y_test_20)
0.9275355587808418
print("Classification Report for Random Forest: \n", classification_report(le.inverse_transform(y_test_20), le.inverse_transform(y_pred_svm)))
Classification Report for Random Forest: 
                precision    recall  f1-score   support

       BENIGN       0.95      0.86      0.90       616
    DrDoS_DNS       0.90      0.89      0.90     14699
   DrDoS_LDAP       0.94      0.95      0.95     15992
  DrDoS_MSSQL       0.86      0.93      0.89     17017
    DrDoS_NTP       1.00      0.96      0.98     17886
DrDoS_NetBIOS       0.87      0.99      0.93     15529
   DrDoS_SNMP       0.98      0.96      0.97     20133
   DrDoS_SSDP       0.99      0.68      0.80     15562
    DrDoS_UDP       0.80      1.00      0.89     18565
          Syn       1.00      1.00      1.00     16340
      UDP-lag       1.00      0.90      0.95     19883
      WebDDoS       0.48      0.57      0.52        28

     accuracy                           0.93    172250
    macro avg       0.90      0.89      0.89    172250
 weighted avg       0.94      0.93      0.93    172250

svm_conf_mat = confusion_matrix(y_test_20, y_pred_svm)
print("SVM Confusion Matrix: \n", svm_conf_mat)
SVM Confusion Matrix: 
 [[  530     0     0    44     6     0     0     0     0    21    13     2]
 [    0 13108   805   348    41   397     0     0     0     0     0     0]
 [    1   730 15226    19     2    12     0     2     0     0     0     0]
 [    1     0   178 15853     0   967     1    16     0     0     0     1]
 [    7   685     2    39 17136     7     0     2     0     8     0     0]
 [    0     0     2   143     0 15380     0     3     0     0     0     1]
 [    0     0     0     3     0   883 19242     1     0     0     0     4]
 [    1     0     0  2059     0    11   273 10518  2700     0     0     0]
 [    1     0     0     0     0     4    32    49 18477     0     0     2]
 [    9     0     0    17     0     0     0     0     0 16314     0     0]
 [    1     0     0     0     0     0    10     0  1897     0 17968     7]
 [    8     0     0     0     0     0     0     0     0     0     4    16]]
acc_score_svm = accuracy_score(y_test_20, y_pred_svm)
print("Accuracy Score for SVM: \n", acc_score_svm*100)
Accuracy Score for SVM: 
 92.75355587808419
# RoC Curve 
# RoC curve Function 

def RoC_Curve_SVM(classifier, X_val, y_val, title): 
        """ RoC Curve for Classifier 
        Parameters: 
        ------------
        classifier: Machine Learning Classifier to be Evaluated
        X_val: Validation Dataset
        y_val: Label/Target of Validation Dataset

        Attributes:
        Plots the Graph    
        
        Note: Some part of this Method code is taken 
            from Sklearn Website
        """

        lw = 2
        n_classes = 12
        y_test1 = to_categorical(y_val)
        pred_RFC_proba = classifier.decision_function(X_val)
        y_score = pred_RFC_proba

        # Compute ROC curve and ROC area for each class
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_test1[:, i], y_score[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        # Compute micro-average ROC curve and ROC area
        fpr["micro"], tpr["micro"], _ = roc_curve(y_test1.ravel(), y_score.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        # First aggregate all false positive rates
        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

        # Then interpolate all ROC curves at this points
        mean_tpr = np.zeros_like(all_fpr)
        for i in range(n_classes):
            mean_tpr += interp(all_fpr, fpr[i], tpr[i])

        # Finally average it and compute AUC
        mean_tpr /= n_classes

        fpr["macro"] = all_fpr
        tpr["macro"] = mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

        # Plot all ROC curves
        plt.figure(figsize=(20,10))
        plt.plot(fpr["micro"], tpr["micro"],
                label='micro-average ROC curve (area = {0:0.2f})'
                    ''.format(roc_auc["micro"]),
                color='deeppink', linestyle=':', linewidth=4)

        plt.plot(fpr["macro"], tpr["macro"],
                label='macro-average ROC curve (area = {0:0.2f})'
                    ''.format(roc_auc["macro"]),
                color='navy', linestyle=':', linewidth=4)

        list_class = ['BENIGN', 'DrDoS_DNS', 'DrDoS_LDAP', 'DrDoS_MSSQL', 'DrDoS_NTP', 'DrDoS_NetBIOS', 'DrDoS_SNMP', 'DrDoS_SSDP', 'DrDoS_UDP', 'Syn', 'UDP-lag', 'WebDDoS']
        for i in range(n_classes):
            plt.plot(fpr[i], tpr[i], lw=lw,
                    label='ROC curve of class {0} (area = {1:0.2f})'
                    ''.format(list_class[i], roc_auc[i]))

        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(title) 
        plt.legend(loc="lower right")
        plt.show()
# RoC Curve 
title = 'Receiver operating characteristic of SVM'
RoC_Curve_SVM(svm, X_test_std_20, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

4. Naive Bayes
from sklearn.naive_bayes import GaussianNB 
gnb = GaussianNB()
gnb.fit(X_train_std_20, y_train_20)
gnb_y_pred = gnb.predict(X_test_std_20)
print("Classification Report for Naive Bayes: \n", classification_report(le.inverse_transform(y_test_20), le.inverse_transform(gnb_y_pred)))
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Classification Report for Naive Bayes: 
                precision    recall  f1-score   support

       BENIGN       0.61      0.96      0.75       616
    DrDoS_DNS       0.63      0.02      0.04     14699
   DrDoS_LDAP       0.54      0.99      0.70     15992
  DrDoS_MSSQL       0.93      0.99      0.96     17017
    DrDoS_NTP       0.99      0.98      0.99     17886
DrDoS_NetBIOS       0.95      0.99      0.97     15529
   DrDoS_SNMP       0.98      0.96      0.97     20133
   DrDoS_SSDP       0.95      0.31      0.47     15562
    DrDoS_UDP       0.58      0.99      0.73     18565
          Syn       1.00      1.00      1.00     16340
      UDP-lag       1.00      0.85      0.92     19883
      WebDDoS       0.00      0.00      0.00        28

     accuracy                           0.82    172250
    macro avg       0.76      0.75      0.71    172250
 weighted avg       0.86      0.82      0.79    172250

gnb_conf_mat = confusion_matrix(y_test_20, gnb_y_pred)
print("Naive Bayes Confusion Matrix: \n", gnb_conf_mat)
Naive Bayes Confusion Matrix: 
 [[  594     0     0     0     0     0     0     0     0    14     8     0]
 [  184   332 13324   694   163     2     0     0     0     0     0     0]
 [    3   176 15770    43     0     0     0     0     0     0     0     0]
 [    3     5   153 16832     1     0     9    14     0     0     0     0]
 [  143     9     0   162 17572     0     0     0     0     0     0     0]
 [    4     0     1   167     0 15342     1    14     0     0     0     0]
 [   11     5     0    10     0   870 19233     4     0     0     0     0]
 [    6     0     0     2     2     0   273  4831 10448     0     0     0]
 [    5     0     0     1     0     0    28   206 18321     0     4     0]
 [    9     0     0    17     0     0     0     0     0 16314     0     0]
 [    4     0     0   152     0     0   123    30  2648     0 16926     0]
 [    0     0     0     0     0     0     0     0     0     0    28     0]]
acc_score_gnb = accuracy_score(y_test_20, gnb_y_pred)
print("Accuracy Score for Naive: \n", acc_score_gnb*100)
Accuracy Score for Naive: 
 82.47721335268506
# RoC Curve 
title = 'Receiver operating characteristic of Naive Bayes'
RoC_Curve(gnb, X_test_std_20, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

5. MLP
from __future__ import print_function
import pandas as pd
import numpy as np
np.random.seed(1337)  # for reproducibility
from keras.preprocessing import sequence
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding
from keras.layers import LSTM, SimpleRNN, GRU
from keras.datasets import imdb
from keras.utils.np_utils import to_categorical
from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)
from sklearn import metrics
from sklearn.preprocessing import Normalizer
import h5py
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
y_train_MLP_20 = np.array(y_train_20)
y_test_MLP_20 = np.array(y_test_20)

y_train_MLP_onehot_20 = to_categorical(y_train_MLP_20)
y_test_MLP_onehot_20 = to_categorical(y_test_MLP_20)

X_train_20_MLP = np.array(X_train_std_20)
X_test_20_MLP = np.array(X_test_std_20)
# y_train_MLP = np.array(y_train)
# y_test_MLP = np.array(y_test)

# y_train_MLP_onehot = to_categorical(y_train_MLP)
# y_test_MLP_onehot = to_categorical(y_test_MLP)

# X_train_MLP = np.array(X_train_std)
# X_test_MLP = np.array(X_test_std)
batch_size = 1000

# 1. define the network
model = Sequential()
model.add(Dense(1024,input_dim=20,activation='relu'))  
model.add(Dropout(0.01))
model.add(Dense(2024,activation='relu'))  
model.add(Dropout(0.01))
# model.add(Dense(3024,activation='relu'))  
# model.add(Dropout(0.01))
# model.add(Dense(2500,activation='relu'))  
# model.add(Dropout(0.01))
model.add(Dense(2000,activation='relu'))  
model.add(Dropout(0.01))
model.add(Dense(1000,activation='relu'))  
model.add(Dropout(0.01))
model.add(Dense(500,activation='relu'))  
model.add(Dropout(0.01))
model.add(Dense(200,activation='relu'))  
model.add(Dropout(0.01))
model.add(Dense(12))
model.add(Activation('softmax'))
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',
      restore_best_weights=True)
# try using different optimizers and different optimizer configs
model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])
model.fit(X_train_20_MLP, y_train_MLP_onehot_20, validation_data=(X_test_20_MLP, y_test_MLP_onehot_20),batch_size=batch_size, epochs=100,callbacks=[monitor])
Epoch 1/100
402/402 [==============================] - 8s 21ms/step - loss: 2.4215 - accuracy: 0.2453 - val_loss: 2.3474 - val_accuracy: 0.3929
Epoch 2/100
402/402 [==============================] - 8s 20ms/step - loss: 2.2612 - accuracy: 0.3967 - val_loss: 2.1426 - val_accuracy: 0.4009
Epoch 3/100
402/402 [==============================] - 8s 21ms/step - loss: 2.0084 - accuracy: 0.4127 - val_loss: 1.8419 - val_accuracy: 0.4742
Epoch 4/100
402/402 [==============================] - 8s 21ms/step - loss: 1.7126 - accuracy: 0.4744 - val_loss: 1.5701 - val_accuracy: 0.4819
Epoch 5/100
402/402 [==============================] - 8s 21ms/step - loss: 1.4820 - accuracy: 0.4998 - val_loss: 1.3719 - val_accuracy: 0.5604
Epoch 6/100
402/402 [==============================] - 9s 21ms/step - loss: 1.2969 - accuracy: 0.6237 - val_loss: 1.1923 - val_accuracy: 0.7113
Epoch 7/100
402/402 [==============================] - 9s 22ms/step - loss: 1.1212 - accuracy: 0.7178 - val_loss: 1.0223 - val_accuracy: 0.7244
Epoch 8/100
402/402 [==============================] - 9s 22ms/step - loss: 0.9663 - accuracy: 0.7349 - val_loss: 0.8854 - val_accuracy: 0.7463
Epoch 9/100
402/402 [==============================] - 9s 23ms/step - loss: 0.8492 - accuracy: 0.7503 - val_loss: 0.7867 - val_accuracy: 0.7538
Epoch 10/100
402/402 [==============================] - 9s 23ms/step - loss: 0.7643 - accuracy: 0.7634 - val_loss: 0.7145 - val_accuracy: 0.7684
Epoch 11/100
402/402 [==============================] - 9s 23ms/step - loss: 0.6997 - accuracy: 0.7771 - val_loss: 0.6572 - val_accuracy: 0.7890
Epoch 12/100
402/402 [==============================] - 9s 23ms/step - loss: 0.6467 - accuracy: 0.7937 - val_loss: 0.6087 - val_accuracy: 0.8103
Epoch 13/100
402/402 [==============================] - 9s 22ms/step - loss: 0.6012 - accuracy: 0.8100 - val_loss: 0.5665 - val_accuracy: 0.8315
Epoch 14/100
402/402 [==============================] - 9s 22ms/step - loss: 0.5608 - accuracy: 0.8269 - val_loss: 0.5287 - val_accuracy: 0.8496
Epoch 15/100
402/402 [==============================] - 9s 22ms/step - loss: 0.5247 - accuracy: 0.8436 - val_loss: 0.4948 - val_accuracy: 0.8616
Epoch 16/100
402/402 [==============================] - 9s 22ms/step - loss: 0.4924 - accuracy: 0.8582 - val_loss: 0.4643 - val_accuracy: 0.8838
Epoch 17/100
402/402 [==============================] - 9s 22ms/step - loss: 0.4637 - accuracy: 0.8721 - val_loss: 0.4370 - val_accuracy: 0.8950
Epoch 18/100
402/402 [==============================] - 9s 22ms/step - loss: 0.4380 - accuracy: 0.8831 - val_loss: 0.4129 - val_accuracy: 0.9008
Epoch 19/100
402/402 [==============================] - 9s 22ms/step - loss: 0.4149 - accuracy: 0.8928 - val_loss: 0.3912 - val_accuracy: 0.9102
Epoch 20/100
402/402 [==============================] - 9s 23ms/step - loss: 0.3942 - accuracy: 0.9002 - val_loss: 0.3713 - val_accuracy: 0.9177
Epoch 21/100
402/402 [==============================] - 9s 22ms/step - loss: 0.3748 - accuracy: 0.9071 - val_loss: 0.3528 - val_accuracy: 0.9226
Epoch 22/100
402/402 [==============================] - 9s 23ms/step - loss: 0.3567 - accuracy: 0.9128 - val_loss: 0.3357 - val_accuracy: 0.9278
Epoch 23/100
402/402 [==============================] - 9s 23ms/step - loss: 0.3404 - accuracy: 0.9171 - val_loss: 0.3197 - val_accuracy: 0.9321
Epoch 24/100
402/402 [==============================] - 9s 23ms/step - loss: 0.3252 - accuracy: 0.9215 - val_loss: 0.3051 - val_accuracy: 0.9344
Epoch 25/100
402/402 [==============================] - 9s 23ms/step - loss: 0.3112 - accuracy: 0.9247 - val_loss: 0.2916 - val_accuracy: 0.9388
Epoch 26/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2981 - accuracy: 0.9284 - val_loss: 0.2791 - val_accuracy: 0.9429
Epoch 27/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2862 - accuracy: 0.9306 - val_loss: 0.2673 - val_accuracy: 0.9436
Epoch 28/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2750 - accuracy: 0.9333 - val_loss: 0.2567 - val_accuracy: 0.9455
Epoch 29/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2648 - accuracy: 0.9352 - val_loss: 0.2467 - val_accuracy: 0.9470
Epoch 30/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2555 - accuracy: 0.9370 - val_loss: 0.2373 - val_accuracy: 0.9482
Epoch 31/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2467 - accuracy: 0.9382 - val_loss: 0.2292 - val_accuracy: 0.9495
Epoch 32/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2390 - accuracy: 0.9395 - val_loss: 0.2216 - val_accuracy: 0.9501
Epoch 33/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2318 - accuracy: 0.9408 - val_loss: 0.2148 - val_accuracy: 0.9513
Epoch 34/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2253 - accuracy: 0.9415 - val_loss: 0.2082 - val_accuracy: 0.9520
Epoch 35/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2192 - accuracy: 0.9427 - val_loss: 0.2023 - val_accuracy: 0.9527
Epoch 36/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2134 - accuracy: 0.9436 - val_loss: 0.1965 - val_accuracy: 0.9534
Epoch 37/100
402/402 [==============================] - 9s 22ms/step - loss: 0.2080 - accuracy: 0.9442 - val_loss: 0.1913 - val_accuracy: 0.9542
Epoch 38/100
402/402 [==============================] - 9s 23ms/step - loss: 0.2031 - accuracy: 0.9447 - val_loss: 0.1868 - val_accuracy: 0.9547
Epoch 39/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1983 - accuracy: 0.9458 - val_loss: 0.1820 - val_accuracy: 0.9554
Epoch 40/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1942 - accuracy: 0.9465 - val_loss: 0.1780 - val_accuracy: 0.9559
Epoch 41/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1905 - accuracy: 0.9470 - val_loss: 0.1740 - val_accuracy: 0.9563
Epoch 42/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1865 - accuracy: 0.9478 - val_loss: 0.1703 - val_accuracy: 0.9569
Epoch 43/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1835 - accuracy: 0.9479 - val_loss: 0.1670 - val_accuracy: 0.9572
Epoch 44/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1796 - accuracy: 0.9489 - val_loss: 0.1637 - val_accuracy: 0.9576
Epoch 45/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1764 - accuracy: 0.9497 - val_loss: 0.1606 - val_accuracy: 0.9584
Epoch 46/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1739 - accuracy: 0.9497 - val_loss: 0.1576 - val_accuracy: 0.9587
Epoch 47/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1707 - accuracy: 0.9504 - val_loss: 0.1549 - val_accuracy: 0.9593
Epoch 48/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1681 - accuracy: 0.9509 - val_loss: 0.1524 - val_accuracy: 0.9596
Epoch 49/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1658 - accuracy: 0.9510 - val_loss: 0.1497 - val_accuracy: 0.9600
Epoch 50/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1635 - accuracy: 0.9517 - val_loss: 0.1473 - val_accuracy: 0.9606
Epoch 51/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1611 - accuracy: 0.9520 - val_loss: 0.1450 - val_accuracy: 0.9612
Epoch 52/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1588 - accuracy: 0.9526 - val_loss: 0.1430 - val_accuracy: 0.9618
Epoch 53/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1573 - accuracy: 0.9528 - val_loss: 0.1407 - val_accuracy: 0.9621
Epoch 54/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1549 - accuracy: 0.9535 - val_loss: 0.1388 - val_accuracy: 0.9622
Epoch 55/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1530 - accuracy: 0.9539 - val_loss: 0.1369 - val_accuracy: 0.9625
Epoch 56/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1508 - accuracy: 0.9549 - val_loss: 0.1352 - val_accuracy: 0.9630
Epoch 57/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1491 - accuracy: 0.9549 - val_loss: 0.1334 - val_accuracy: 0.9634
Epoch 58/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1475 - accuracy: 0.9554 - val_loss: 0.1316 - val_accuracy: 0.9637
Epoch 59/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1459 - accuracy: 0.9553 - val_loss: 0.1300 - val_accuracy: 0.9637
Epoch 60/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1444 - accuracy: 0.9557 - val_loss: 0.1284 - val_accuracy: 0.9641
Epoch 61/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1429 - accuracy: 0.9562 - val_loss: 0.1271 - val_accuracy: 0.9642
Epoch 62/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1412 - accuracy: 0.9567 - val_loss: 0.1255 - val_accuracy: 0.9652
Epoch 63/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1399 - accuracy: 0.9571 - val_loss: 0.1243 - val_accuracy: 0.9650
Epoch 64/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1386 - accuracy: 0.9574 - val_loss: 0.1229 - val_accuracy: 0.9655
Epoch 65/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1374 - accuracy: 0.9577 - val_loss: 0.1215 - val_accuracy: 0.9658
Epoch 66/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1361 - accuracy: 0.9577 - val_loss: 0.1203 - val_accuracy: 0.9663
Epoch 67/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1351 - accuracy: 0.9579 - val_loss: 0.1190 - val_accuracy: 0.9662
Epoch 68/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1340 - accuracy: 0.9583 - val_loss: 0.1179 - val_accuracy: 0.9672
Epoch 69/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1324 - accuracy: 0.9591 - val_loss: 0.1167 - val_accuracy: 0.9671
Epoch 70/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1314 - accuracy: 0.9592 - val_loss: 0.1157 - val_accuracy: 0.9676
Epoch 71/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1302 - accuracy: 0.9595 - val_loss: 0.1147 - val_accuracy: 0.9677
Epoch 72/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1294 - accuracy: 0.9593 - val_loss: 0.1134 - val_accuracy: 0.9689
Epoch 73/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1286 - accuracy: 0.9600 - val_loss: 0.1124 - val_accuracy: 0.9683
Epoch 74/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1276 - accuracy: 0.9600 - val_loss: 0.1116 - val_accuracy: 0.9683
Epoch 75/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1261 - accuracy: 0.9608 - val_loss: 0.1104 - val_accuracy: 0.9693
Epoch 76/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1255 - accuracy: 0.9609 - val_loss: 0.1097 - val_accuracy: 0.9694
Epoch 77/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1242 - accuracy: 0.9612 - val_loss: 0.1087 - val_accuracy: 0.9696
Epoch 78/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1235 - accuracy: 0.9617 - val_loss: 0.1078 - val_accuracy: 0.9696
Epoch 79/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1228 - accuracy: 0.9614 - val_loss: 0.1070 - val_accuracy: 0.9694
Epoch 80/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1222 - accuracy: 0.9619 - val_loss: 0.1062 - val_accuracy: 0.9695
Epoch 81/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1208 - accuracy: 0.9618 - val_loss: 0.1053 - val_accuracy: 0.9696
Epoch 82/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1199 - accuracy: 0.9622 - val_loss: 0.1045 - val_accuracy: 0.9698
Epoch 83/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1197 - accuracy: 0.9622 - val_loss: 0.1036 - val_accuracy: 0.9716
Epoch 84/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1184 - accuracy: 0.9626 - val_loss: 0.1028 - val_accuracy: 0.9717
Epoch 85/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1180 - accuracy: 0.9625 - val_loss: 0.1021 - val_accuracy: 0.9721
Epoch 86/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1174 - accuracy: 0.9626 - val_loss: 0.1015 - val_accuracy: 0.9722
Epoch 87/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1162 - accuracy: 0.9633 - val_loss: 0.1006 - val_accuracy: 0.9717
Epoch 88/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1157 - accuracy: 0.9632 - val_loss: 0.1000 - val_accuracy: 0.9722
Epoch 89/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1151 - accuracy: 0.9635 - val_loss: 0.0994 - val_accuracy: 0.9719
Epoch 90/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1143 - accuracy: 0.9638 - val_loss: 0.0988 - val_accuracy: 0.9717
Epoch 91/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1139 - accuracy: 0.9638 - val_loss: 0.0979 - val_accuracy: 0.9726
Epoch 92/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1134 - accuracy: 0.9637 - val_loss: 0.0975 - val_accuracy: 0.9721
Epoch 93/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1123 - accuracy: 0.9644 - val_loss: 0.0969 - val_accuracy: 0.9720
Epoch 94/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1122 - accuracy: 0.9639 - val_loss: 0.0963 - val_accuracy: 0.9724
Epoch 95/100
402/402 [==============================] - 9s 23ms/step - loss: 0.1113 - accuracy: 0.9644 - val_loss: 0.0957 - val_accuracy: 0.9728
Epoch 96/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1109 - accuracy: 0.9647 - val_loss: 0.0954 - val_accuracy: 0.9725
Epoch 97/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1102 - accuracy: 0.9649 - val_loss: 0.0943 - val_accuracy: 0.9744
Epoch 98/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1097 - accuracy: 0.9649 - val_loss: 0.0938 - val_accuracy: 0.9744
Epoch 99/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1091 - accuracy: 0.9652 - val_loss: 0.0935 - val_accuracy: 0.9740
Epoch 100/100
402/402 [==============================] - 9s 22ms/step - loss: 0.1086 - accuracy: 0.9652 - val_loss: 0.0931 - val_accuracy: 0.9733
<tensorflow.python.keras.callbacks.History at 0x7f912e000048>
y_pred_MLP = model.predict_classes(X_test_20_MLP)
WARNING:tensorflow:From <ipython-input-88-afd9bc9ca0a1>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.
Instructions for updating:
Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
y_pred_MLP
array([ 4,  5,  4, ..., 10,  2, 10])
print("Classification Report for MLP: \n", classification_report(le.inverse_transform(y_test_MLP_20), le.inverse_transform(y_pred_MLP)))
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Classification Report for MLP: 
                precision    recall  f1-score   support

       BENIGN       0.97      0.87      0.92       616
    DrDoS_DNS       0.98      0.96      0.97     14699
   DrDoS_LDAP       0.98      0.98      0.98     15992
  DrDoS_MSSQL       0.99      0.99      0.99     17017
    DrDoS_NTP       0.98      1.00      0.99     17886
DrDoS_NetBIOS       0.94      0.99      0.97     15529
   DrDoS_SNMP       0.98      0.96      0.97     20133
   DrDoS_SSDP       0.97      0.97      0.97     15562
    DrDoS_UDP       0.93      0.96      0.95     18565
          Syn       1.00      1.00      1.00     16340
      UDP-lag       0.99      0.94      0.97     19883
      WebDDoS       0.00      0.00      0.00        28

     accuracy                           0.97    172250
    macro avg       0.89      0.88      0.89    172250
 weighted avg       0.97      0.97      0.97    172250

mlp_conf_mat = confusion_matrix(y_test_20, y_pred_MLP)
print("MLP Confusion: \n", mlp_conf_mat)
MLP Confusion: 
 [[  536     2     0     0    15     1     0     1     4    42    15     0]
 [    0 14108   162     7   418     4     0     0     0     0     0     0]
 [    1   287 15690    12     0     0     0     0     0     2     0     0]
 [    1     5   142 16805     0    48    15     0     0     1     0     0]
 [    4    62     0     2 17804     0     0     0     0    14     0     0]
 [    0     0     0   154     0 15369     3     0     0     3     0     0]
 [    2     0     0     0     0   880 19248     3     0     0     0     0]
 [    1     0     0     0     0     0   273 15134   150     0     4     0]
 [    2     0     0     0     0     0    31   454 17904     0   174     0]
 [    7     0     0     0    17     0     0     0     0 16316     0     0]
 [    0     0     0     0     0     0     0     0  1138     0 18745     0]
 [    0     0     0     0     0     0     0     0     0     0    28     0]]
acc_score_mlp = accuracy_score(y_test_20, y_pred_MLP)
print("Accuracy Score for MLP: \n", acc_score_mlp*100)
Accuracy Score for MLP: 
 97.33468795355587
# RoC Curve 
title = 'Receiver operating characteristic of MultiLayer Perceptron'
RoC_Curve(model, X_test_std_20, y_test_20, title)
WARNING:tensorflow:From <ipython-input-59-ef3132ab84e8>:21: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.
Instructions for updating:
Please use `model.predict()` instead.
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

6. LSTM
y_train_lstm_20 = np.array(y_train_20)
y_test_lstm_20 = np.array(y_test_20)

y_train_onehot_lstm = to_categorical(y_train_lstm_20)
y_test_one_hot_lstm = to_categorical(y_test_lstm_20)

X_train_lstm_20 = np.array(X_train_std_20)
X_test_lstm_20 = np.array(X_test_std_20)
X_test_std_20
array([[-1.05407664, -0.90181963, -0.30345804, ...,  3.22386189,
         0.79641513,  3.23300538],
       [ 0.24816089, -0.88751843, -0.67651841, ..., -0.3061992 ,
        -1.57219156, -0.24475475],
       [-1.23226315, -0.89522235, -0.30345804, ...,  1.0835465 ,
        -1.5264022 ,  1.10376449],
       ...,
       [ 1.64529686,  1.29307531, -1.08140384, ..., -0.34333042,
         0.80340264, -0.2802421 ],
       [-0.69570018, -0.90118119,  1.52117848, ..., -0.10465283,
         0.92409597, -0.24475475],
       [ 1.67631415, -0.2734603 , -1.08140384, ..., -0.34333042,
        -0.31645152, -0.2802421 ]])
X_train_lstm_20.shape[0] 
401914
X_train_lstm_reshape = np.reshape(X_train_std_20, (X_train_lstm_20.shape[0], 1,  X_train_lstm_20.shape[1]))
X_test_lstm_reshape = np.reshape(X_test_std_20, (X_test_lstm_20.shape[0], 1, X_test_lstm_20.shape[1]))
 batch_size = 1000

# Initialize the network
model_LSTM = Sequential()
model_LSTM.add(LSTM(8,input_dim=20, return_sequences=True)) 
model_LSTM.add(Dropout(0.1))
model_LSTM.add(LSTM(8,input_dim=20, return_sequences=False))
model_LSTM.add(Dropout(0.1))
model_LSTM.add(Dense(12))
model_LSTM.add(Activation('softmax'))
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',
      restore_best_weights=True)
model_LSTM.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model_LSTM.fit(X_train_lstm_reshape, y_train_onehot_lstm, validation_data=(X_test_lstm_reshape, y_test_one_hot_lstm),batch_size=batch_size, epochs=50,callbacks=[monitor])
Epoch 1/50
402/402 [==============================] - 3s 8ms/step - loss: 1.9700 - accuracy: 0.4289 - val_loss: 1.3623 - val_accuracy: 0.5652
Epoch 2/50
402/402 [==============================] - 3s 6ms/step - loss: 1.0693 - accuracy: 0.6210 - val_loss: 0.7698 - val_accuracy: 0.6990
Epoch 3/50
402/402 [==============================] - 2s 6ms/step - loss: 0.7621 - accuracy: 0.6899 - val_loss: 0.6236 - val_accuracy: 0.7300
Epoch 4/50
402/402 [==============================] - 2s 6ms/step - loss: 0.6189 - accuracy: 0.7746 - val_loss: 0.4546 - val_accuracy: 0.8584
Epoch 5/50
402/402 [==============================] - 2s 6ms/step - loss: 0.4894 - accuracy: 0.8481 - val_loss: 0.3271 - val_accuracy: 0.9468
Epoch 6/50
402/402 [==============================] - 2s 6ms/step - loss: 0.3813 - accuracy: 0.8987 - val_loss: 0.2290 - val_accuracy: 0.9566
Epoch 7/50
402/402 [==============================] - 3s 6ms/step - loss: 0.3120 - accuracy: 0.9151 - val_loss: 0.1836 - val_accuracy: 0.9610
Epoch 8/50
402/402 [==============================] - 3s 6ms/step - loss: 0.2726 - accuracy: 0.9235 - val_loss: 0.1569 - val_accuracy: 0.9613
Epoch 9/50
402/402 [==============================] - 3s 6ms/step - loss: 0.2485 - accuracy: 0.9281 - val_loss: 0.1396 - val_accuracy: 0.9637
Epoch 10/50
402/402 [==============================] - 3s 6ms/step - loss: 0.2280 - accuracy: 0.9334 - val_loss: 0.1282 - val_accuracy: 0.9656
Epoch 11/50
402/402 [==============================] - 3s 7ms/step - loss: 0.2106 - accuracy: 0.9370 - val_loss: 0.1178 - val_accuracy: 0.9652
Epoch 12/50
402/402 [==============================] - 3s 7ms/step - loss: 0.1950 - accuracy: 0.9407 - val_loss: 0.1103 - val_accuracy: 0.9664
Epoch 13/50
402/402 [==============================] - 3s 7ms/step - loss: 0.1840 - accuracy: 0.9431 - val_loss: 0.1035 - val_accuracy: 0.9668
Epoch 14/50
402/402 [==============================] - 3s 7ms/step - loss: 0.1752 - accuracy: 0.9453 - val_loss: 0.0978 - val_accuracy: 0.9676
Epoch 15/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1686 - accuracy: 0.9470 - val_loss: 0.0952 - val_accuracy: 0.9666
Epoch 16/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1620 - accuracy: 0.9488 - val_loss: 0.0918 - val_accuracy: 0.9663
Epoch 17/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1572 - accuracy: 0.9494 - val_loss: 0.0876 - val_accuracy: 0.9689
Epoch 18/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1517 - accuracy: 0.9515 - val_loss: 0.0843 - val_accuracy: 0.9695
Epoch 19/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1481 - accuracy: 0.9528 - val_loss: 0.0836 - val_accuracy: 0.9690
Epoch 20/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1438 - accuracy: 0.9544 - val_loss: 0.0807 - val_accuracy: 0.9697
Epoch 21/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1410 - accuracy: 0.9550 - val_loss: 0.0790 - val_accuracy: 0.9708
Epoch 22/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1388 - accuracy: 0.9558 - val_loss: 0.0787 - val_accuracy: 0.9706
Epoch 23/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1361 - accuracy: 0.9567 - val_loss: 0.0747 - val_accuracy: 0.9733
Epoch 24/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1337 - accuracy: 0.9577 - val_loss: 0.0736 - val_accuracy: 0.9729
Epoch 25/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1325 - accuracy: 0.9579 - val_loss: 0.0717 - val_accuracy: 0.9742
Epoch 26/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1296 - accuracy: 0.9590 - val_loss: 0.0721 - val_accuracy: 0.9736
Epoch 27/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1273 - accuracy: 0.9593 - val_loss: 0.0712 - val_accuracy: 0.9738
Epoch 28/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1259 - accuracy: 0.9600 - val_loss: 0.0679 - val_accuracy: 0.9763
Epoch 29/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1235 - accuracy: 0.9608 - val_loss: 0.0665 - val_accuracy: 0.9773
Epoch 30/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1223 - accuracy: 0.9609 - val_loss: 0.0680 - val_accuracy: 0.9749
Epoch 31/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1209 - accuracy: 0.9617 - val_loss: 0.0672 - val_accuracy: 0.9746
Epoch 32/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1187 - accuracy: 0.9623 - val_loss: 0.0650 - val_accuracy: 0.9759
Epoch 33/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1176 - accuracy: 0.9628 - val_loss: 0.0640 - val_accuracy: 0.9755
Epoch 34/50
402/402 [==============================] - 3s 7ms/step - loss: 0.1164 - accuracy: 0.9631 - val_loss: 0.0629 - val_accuracy: 0.9775
Epoch 35/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1145 - accuracy: 0.9638 - val_loss: 0.0612 - val_accuracy: 0.9811
Epoch 36/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1136 - accuracy: 0.9643 - val_loss: 0.0610 - val_accuracy: 0.9782
Epoch 37/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1132 - accuracy: 0.9644 - val_loss: 0.0634 - val_accuracy: 0.9762
Epoch 38/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1118 - accuracy: 0.9649 - val_loss: 0.0595 - val_accuracy: 0.9788
Epoch 39/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1109 - accuracy: 0.9652 - val_loss: 0.0589 - val_accuracy: 0.9796
Epoch 40/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1104 - accuracy: 0.9651 - val_loss: 0.0585 - val_accuracy: 0.9783
Epoch 41/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1086 - accuracy: 0.9658 - val_loss: 0.0578 - val_accuracy: 0.9799
Epoch 42/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1084 - accuracy: 0.9662 - val_loss: 0.0578 - val_accuracy: 0.9806
Epoch 43/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1070 - accuracy: 0.9665 - val_loss: 0.0598 - val_accuracy: 0.9793
Epoch 44/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1061 - accuracy: 0.9666 - val_loss: 0.0566 - val_accuracy: 0.9811
Epoch 45/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1051 - accuracy: 0.9672 - val_loss: 0.0573 - val_accuracy: 0.9810
Epoch 46/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1047 - accuracy: 0.9669 - val_loss: 0.0536 - val_accuracy: 0.9838
Epoch 47/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1046 - accuracy: 0.9671 - val_loss: 0.0555 - val_accuracy: 0.9815
Epoch 48/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1036 - accuracy: 0.9677 - val_loss: 0.0532 - val_accuracy: 0.9832
Epoch 49/50
402/402 [==============================] - 3s 6ms/step - loss: 0.1031 - accuracy: 0.9676 - val_loss: 0.0539 - val_accuracy: 0.9824
Epoch 50/50
402/402 [==============================] - 2s 6ms/step - loss: 0.1013 - accuracy: 0.9682 - val_loss: 0.0537 - val_accuracy: 0.9816
<tensorflow.python.keras.callbacks.History at 0x7f912008d748>
y_perd_lstm = model_LSTM.predict_classes(X_test_lstm_reshape)
print("Classification Report for LSTM: \n", classification_report(le.inverse_transform(y_test_lstm_20), le.inverse_transform(y_perd_lstm)))
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Classification Report for LSTM: 
                precision    recall  f1-score   support

       BENIGN       0.94      0.85      0.89       616
    DrDoS_DNS       0.99      0.97      0.98     14699
   DrDoS_LDAP       0.97      0.99      0.98     15992
  DrDoS_MSSQL       0.99      0.99      0.99     17017
    DrDoS_NTP       1.00      1.00      1.00     17886
DrDoS_NetBIOS       0.95      0.99      0.97     15529
   DrDoS_SNMP       0.98      0.96      0.97     20133
   DrDoS_SSDP       0.98      0.98      0.98     15562
    DrDoS_UDP       0.96      0.98      0.97     18565
          Syn       1.00      1.00      1.00     16340
      UDP-lag       0.99      0.96      0.98     19883
      WebDDoS       0.00      0.00      0.00        28

     accuracy                           0.98    172250
    macro avg       0.90      0.89      0.89    172250
 weighted avg       0.98      0.98      0.98    172250

lstm_conf_mat = confusion_matrix(y_test_lstm_20, y_perd_lstm)
print("LSTM Confusion: \n", lstm_conf_mat)
LSTM Confusion: 
 [[  521     0     0     0     5    12    13     0     1    50    14     0]
 [    0 14265   386     0    46     0     2     0     0     0     0     0]
 [    0   121 15861     1     0     0     3     0     0     6     0     0]
 [    1     0    92 16922     0     0     0     1     0     1     0     0]
 [    1    15     0     0 17867     0     0     0     0     3     0     0]
 [    1     0     0   165     0 15352     8     0     0     3     0     0]
 [    3     0     0     1     0   852 19273     2     1     0     1     0]
 [    1     0     0     0     0     0   266 15266    10     0    19     0]
 [    2     0     0     0     0     0     4   251 18244     0    64     0]
 [    7     0     0     0     2     0     0     0     0 16331     0     0]
 [    7     0     0     0     0     0     0     0   700     0 19176     0]
 [   12     0     0     0     0     0     0     0     0     0    16     0]]
acc_score_lstm = accuracy_score(y_test_lstm_20, y_perd_lstm)
print("Accuracy Score for MLP: \n", acc_score_lstm*100)
Accuracy Score for MLP: 
 98.15849056603774
# RoC Curve 
title = 'Receiver operating characteristic of LSTM'
RoC_Curve(model_LSTM, X_test_lstm_reshape, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

7. XGBoost
from sklearn.ensemble import GradientBoostingClassifier
# fit model no training data
gradinet_boost = GradientBoostingClassifier()
gradinet_boost.fit(X_train_std_20, y_train_20)
GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_iter_no_change=None, presort='deprecated',
                           random_state=None, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
# Predict the labels 
y_pred_xgboost = gradinet_boost.predict(X_test_std_20)
y_pred_xgboost
array([ 4,  6,  4, ..., 10,  2, 10])
y_test_20
array([ 4,  6,  4, ..., 10,  2, 10])
# Accuracy Score 
print("Accuracy Score for the XGBoost Classifier is: {0:.3f}%".format(accuracy_score(y_test_20, y_pred_xgboost)* 100))
Accuracy Score for the XGBoost Classifier is: 98.592%
# Classification Report 
print("Classification Report for XGBOOST: \n", classification_report(le.inverse_transform(y_test_20), le.inverse_transform(y_pred_xgboost)))
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Classification Report for XGBOOST: 
                precision    recall  f1-score   support

       BENIGN       0.47      0.45      0.46       616
    DrDoS_DNS       0.98      0.98      0.98     14699
   DrDoS_LDAP       0.98      0.98      0.98     15992
  DrDoS_MSSQL       0.98      0.99      0.98     17017
    DrDoS_NTP       0.99      0.99      0.99     17886
DrDoS_NetBIOS       0.99      0.98      0.99     15529
   DrDoS_SNMP       0.99      0.98      0.99     20133
   DrDoS_SSDP       0.98      0.99      0.98     15562
    DrDoS_UDP       0.99      0.99      0.99     18565
          Syn       1.00      0.99      1.00     16340
      UDP-lag       1.00      0.99      0.99     19883
      WebDDoS       0.00      0.00      0.00        28

     accuracy                           0.99    172250
    macro avg       0.86      0.86      0.86    172250
 weighted avg       0.99      0.99      0.99    172250

# Confusion Matrix 
xgboost_conf_mat = confusion_matrix(y_test_20, y_pred_xgboost)
print("LSTM Confusion: \n", xgboost_conf_mat)
LSTM Confusion: 
 [[  278    30   229    63     0     9     5     0     0     0     2     0]
 [   38 14422     0    27   196     0     0    16     0     0     0     0]
 [   34   207 15709    32     0     0     0     9     1     0     0     0]
 [    0     2    83 16920     0     0     0    11     1     0     0     0]
 [   36     0     0    35 17780     0     0    18    11     6     0     0]
 [   43     0     1   183     0 15289     0    10     3     0     0     0]
 [   54     8     1    36     0   186 19826    19     1     2     0     0]
 [    4     0     0     0     0     0   177 15381     0     0     0     0]
 [    0     0     0     1     0     0     0   173 18391     0     0     0]
 [   95     0     0    15     0     0    29    14     0 16187     0     0]
 [    5     0     0    27     0     0     0    18   192     0 19641     0]
 [    4     0     0    14     0     0     0     0     7     0     3     0]]
# RoC Curve 
title = 'Receiver operating characteristic of XGBOOST'
RoC_Curve(gradinet_boost, X_test_std_20, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

Ensemble Method of Machine Learning
from sklearn.pipeline import Pipeline 
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import cross_val_score
# ADABOOST 
adaboost = AdaBoostClassifier(base_estimator= dt, n_estimators=100)
adaboost.fit(X_train_std_20, y_train_20)
AdaBoostClassifier(algorithm='SAMME.R',
                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,
                                                         class_weight=None,
                                                         criterion='gini',
                                                         max_depth=None,
                                                         max_features=None,
                                                         max_leaf_nodes=None,
                                                         min_impurity_decrease=0.0,
                                                         min_impurity_split=None,
                                                         min_samples_leaf=1,
                                                         min_samples_split=2,
                                                         min_weight_fraction_leaf=0.0,
                                                         presort='deprecated',
                                                         random_state=None,
                                                         splitter='best'),
                   learning_rate=1.0, n_estimators=100, random_state=None)
y_pred_adaboost = adaboost.predict(X_test_std_20)
print("Accuracy Score for Adaboost: ", accuracy_score(y_test_20, y_pred_adaboost))
Accuracy Score for Adaboost:  0.9899680696661829
print("Classification Report for Adaboost: ",classification_report(le.inverse_transform(y_test_20), le.inverse_transform(y_pred_adaboost)))
Classification Report for Adaboost:                 precision    recall  f1-score   support

       BENIGN       0.82      1.00      0.90       616
    DrDoS_DNS       0.99      0.99      0.99     14699
   DrDoS_LDAP       0.98      0.99      0.99     15992
  DrDoS_MSSQL       0.99      0.98      0.99     17017
    DrDoS_NTP       0.99      1.00      0.99     17886
DrDoS_NetBIOS       0.99      0.99      0.99     15529
   DrDoS_SNMP       0.99      0.99      0.99     20133
   DrDoS_SSDP       0.99      0.99      0.99     15562
    DrDoS_UDP       0.99      0.99      0.99     18565
          Syn       1.00      0.99      1.00     16340
      UDP-lag       1.00      0.99      0.99     19883
      WebDDoS       0.44      0.25      0.32        28

     accuracy                           0.99    172250
    macro avg       0.93      0.93      0.93    172250
 weighted avg       0.99      0.99      0.99    172250

# RoC Curve 
title = 'Receiver operating characteristic of ADABOOST'
RoC_Curve(adaboost, X_test_std_20, y_test_20, title)
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead

# Confusion Matrix 
adaboost_conf_mat = confusion_matrix(y_test_20, y_pred_adaboost)
print("Adaboost Confusion: \n", adaboost_conf_mat)
Adaboost Confusion: 
 [[  616     0     0     0     0     0     0     0     0     0     0     0]
 [    0 14502     0     0   197     0     0     0     0     0     0     0]
 [    0   204 15788     0     0     0     0     0     0     0     0     0]
 [    0     0   268 16749     0     0     0     0     0     0     0     0]
 [   38     0     0     0 17831     0     0     0     0    17     0     0]
 [    1     0     0   159     0 15369     0     0     0     0     0     0]
 [    0     0     0     0     0   186 19947     0     0     0     0     0]
 [    0     0     0     0     0     0   181 15381     0     0     0     0]
 [    0     0     0     0     0     0     0   168 18397     0     0     0]
 [   96     0     0     0     0     0     0     0     0 16244     0     0]
 [    0     0     0     0     0     0     0     0   183     0 19691     9]
 [    0     0     0     0     0     0     0     0     0     0    21     7]]
 
# Generating output for each of the classifier for Comparision with Ensemble learning 
clf_labels = ['Decision Tree', 'SVM']
# for clf, label in zip([dt, svm, gradinet_boost, adaboost], clf_labels): 
#   scores = cross_val_score(estimator=clf, X = X_train_std_20, y = y_train_20, cv = 20, scoring = 'accuracy')
#   print("Accuracy: %0.2f (+/- %0.2f) [%s]" %(scores.mean(), scores.std() * 2, label))
# Using Majority Voting Technique for Ensemble classification 

from sklearn.base import BaseEstimator
from sklearn.base import ClassifierMixin
from sklearn.preprocessing import LabelEncoder
from sklearn.base import clone
from sklearn.pipeline import _name_estimators
import numpy as np
import operator
class MajorityVoteClassifier(BaseEstimator,
                             ClassifierMixin):
    """ A majority vote ensemble classifier
    
    Parameters
    ----------
    classifiers : array-like, shape = [n_classifiers]
      Different classifiers for the ensemble
    
    vote : str, {'classlabel', 'probability'}
      Default: 'classlabel'
      If 'classlabel' the prediction is based on
      the argmax of class labels. Else if
      'probability', the argmax of the sum of
      probabilities is used to predict the class label
      (recommended for calibrated classifiers).
    
    weights : array-like, shape = [n_classifiers]
      Optional, default: None
      If a list of `int` or `float` values are
      provided, the classifiers are weighted by
      importance; Uses uniform weights if `weights=None`.
    
    """
    def __init__(self, classifiers,
                 vote='classlabel', weights=None):
      
        
        self.classifiers = classifiers
        self.named_classifiers = {key: value for
                                  key, value in
                                  _name_estimators(classifiers)}
        self.vote = vote
        self.weights = weights
        
    def fit(self, X, y):
        """ Fit classifiers.
        
        Parameters
        ----------
        X : {array-like, sparse matrix},
            shape = [n_examples, n_features]
            Matrix of training examples.
        
        y : array-like, shape = [n_examples]
            Vector of target class labels.
        
        Returns
        -------
        self : object
        
        """
        if self.vote not in ('probability', 'classlabel'):
            raise ValueError("vote must be 'probability'"
                             "or 'classlabel'; got (vote=%r)"
                             % self.vote)
        if self.weights and len(self.weights) != len(self.classifiers):
          raise ValueError("Number of classifiers and weights"
                             "must be equal; got %d weights,"
                             "%d classifiers"
                             % (len(self.weights),
                             len(self.classifiers)))
        # Use LabelEncoder to ensure class labels start
        # with 0, which is important for np.argmax
        # call in self.predict
        self.lablenc_ = LabelEncoder()
        self.lablenc_.fit(y)
        self.classes_ = self.lablenc_.classes_
        self.classifiers_ = []
        for clf in self.classifiers:
            fitted_clf = clone(clf).fit(X,
                               self.lablenc_.transform(y))
            self.classifiers_.append(fitted_clf)
        return self
    def predict(self, X):

        """ Predict class labels for X.
        
        Parameters
        ----------
        X : {array-like, sparse matrix},
            Shape = [n_examples, n_features]
            Matrix of training examples.
        
        Returns
        ----------
        maj_vote : array-like, shape = [n_examples]
            Predicted class labels.
        
        """
        if self.vote == 'probability':
            maj_vote = np.argmax(self.predict_proba(X), axis=1)
        else: # 'classlabel' vote
            
            # Collect results from clf.predict calls
            predictions = np.asarray([clf.predict(X)
                                      for clf in
                                      self.classifiers_]).T
            
            maj_vote = np.apply_along_axis(lambda x: np.argmax(
                                           np.bincount(x,
                                           weights=self.weights)),
                                           axis=1,
                                           arr=predictions)
        maj_vote = self.lablenc_.inverse_transform(maj_vote)
        return maj_vote
    
    def predict_proba(self, X):
        """ Predict class probabilities for X.
        
        Parameters
        ----------
        X : {array-like, sparse matrix}, 
            shape = [n_examples, n_features]
            Training vectors, where
            n_examples is the number of examples and
            n_features is the number of features.
        
        Returns
        ----------
        avg_proba : array-like,
            shape = [n_examples, n_classes]
            Weighted average probability for
            each class per example.
        
        """
        probas = np.asarray([clf.predict_proba(X)
                             for clf in self.classifiers_])
        avg_proba = np.average(probas, axis=0,
                               weights=self.weights)
        return avg_proba
    
    def get_params(self, deep=True):
        """ Get classifier parameter names for GridSearch"""
        if not deep:
            return super(MajorityVoteClassifier, 
                           self).get_params(deep=False)
        else:
            out = self.named_classifiers.copy()
            for name, step in self.named_classifiers.items():
                for key, value in step.get_params(
                        deep=True).items():
                    out['%s__%s' % (name, key)] = value
            return out
mv_clf = MajorityVoteClassifier(classifiers = [dt, svm, gradinet_boost, adaboost], vote='classlabel')
clf_labels += ['Majority Voting']
all_clf = [dt, svm, mv_clf]
for clf, label in zip(all_clf, clf_labels):
  scores = cross_val_score(estimator=clf, X = X_train_std_20, y= y_train_20, cv = 10, scoring = 'accuracy')
  print("Accuracy Score: %0.2f (+/- %0.2f) [%s]" %(scores.mean(), scores.std()*2, label))
Accuracy Score: 1.00 (+/- 0.00) [Decision Tree]
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
Accuracy Score: 0.93 (+/- 0.00) [SVM]
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
Accuracy Score: 1.00 (+/- 0.00) [Majority Voting]
Ouptut for
mv_clf = MajorityVoteClassifier(classifiers = [dt, svm, gradinet_boost, adaboost], vote='classlabel') clf_labels += ['Majority Voting'] all_clf = [dt, svm, mv_clf]

output:

Accuracy Score: 1.00 (+/- 0.00) [Decision Tree]
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning)

Accuracy Score: 0.93 (+/- 0.00) [SVM]
/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning) /usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. "the number of iterations.", ConvergenceWarning)

Accuracy Score: 1.00 (+/- 0.00) [Majority Voting]
 
